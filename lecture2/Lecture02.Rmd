---
title: "Data Input, Management and Output"
author: "Jeremy Oldfather"
date: "September 9, 2016"
output: html_document
---

## Lecture 2: Data Formats

What is data? How we answer this question dictates the formats we should learn to read, write, and analyze with R. 

### Structured

#### Delimited

The most widely used format for spreadsheet-type data is a text file where the columns of the spreadsheet are delimited by a specific character--usually a comma (`.csv`), tab (`.tsv` or `.txt`), or pipe **|** (`.pip` or `.txt`).

##### Reading / Writing

R has built-in functions to read and write delimited files: `read.csv()`, `read.table()`, `read.delim()`. However, there are better versions of these available in the package `readr`: `read_csv()`, `read_table()`, `read_delim()`. 
We can read local files from our computers or remotely from URLs. Let's read a csv of unemployment data from github.

```{r}
library(readr)
unemp<-read_csv("https://raw.githubusercontent.com/datasets/us-employment-bls/master/data/aat1.csv")
```

`readr` gives us feedback about how it parsed the csv. For example, it recognized that the `Year` column only contained integer values and decided to store that column as an integer vector. This saves space in memory and removes the need for us to do this manually. 

The variable names in the csv, though very descriptive, are not friendly to type. So we might like to rename the variables then save the data and a codebook to disk for later use.

```{r}
# new names for the columns
short_names<-c('year',
               'civ_noninst_pop',
               'civ_lf_tot',
               'pct_pop',
               'emp_tot',
               'emp_pct_pop',
               'emp_ag_tot',
               'emp_nonag_tot',
               'unemp',
               'unemp_pct_lf',
               'not_in_lf',
               'footnotes')
               
# create a codebook with the new names, old names, and the type of each column
codebook<-data.frame(name=short_names,
                     descripition=names(unemp),
                     type=as.character(lapply(unemp,typeof)))
                     
# now rename the variables
names(unemp)<-short_names

# write both the codebook and rename data to disk as tab-delimited files
write_tsv(codebook,"data/unemp_codebook.tsv")
write_tsv(unemp,"data/unemp.tsv")
```

Now that it is saved, let's keep working with the data and see what we else can do. The `dplyr` package is an excellent way to manipulate data frame and summarize data.

##### Exploring with dplyr and ggplot2

```{r}
library(dplyr)
unemp %>% summarise(first=min(year),last=max(year))
```

The dplyr `%>%` notation means, take the output of the thing on the left and pass it to the thing on the right. It is a fancy way of doing function composition. Instead of writing `f(g(h(x)))` we can write `x %>% h() %>% g() %>% f()`. Writing it this way makes it easier to remove `g()` if we don't want it to be part of the pipeline of operations anymore.

We can also plot the unemployment rate easily with the `ggplot2` package. I give you the following code without much explanation, but it will be covered in a later lesson.

```{r, fig.align="center"}
library(ggplot2)
unemp %>% 
  ggplot(aes(year,unemp_pct_lf)) + 
  geom_line() + labs(y="Unemployment Rate",x="")
```

By slightly altering the code above, we can also plot the share of those employed in agriculture. We would expect to see it drop over time.

```{r, fig.align="center"}
unemp %>% 
  ggplot(aes(year,emp_ag_tot/emp_tot)) + 
  geom_line() + labs(y="Share Employed in Agriculture",x="")
```

### Semi-Structured

Semi-structured dataset are those do not cleanly fit in a spreadsheet format. These sources are now widely available across the internet. Examples include feeds from sites like Facebook, Twitter, and Google. Even city governments are beginning to provide data feeds of services like transportation, crime, construction, etc. 

#### JSON

WMATA is an example of a local service that provides a data feed. For detailed info, see [developer.wmata.com](https://developer.wmata.com).

One of the many things WMATA allows us to do is get informations on [bus positions](https://developer.wmata.com/docs/services/54763629281d83086473f231/operations/5476362a281d830c946a3d68). We saw above that the `readr` class of functions allows us to get data from URLs. However, they do not allow us to customize the query, which is something we need since most APIs require us to pass an authorization key. For this, we can use the `httr` packages.

Let's find out where all the 70 buses are right now.

```{r}
library(httr)
resp<-GET("https://api.wmata.com/Bus.svc/json/jBusPositions",
          add_headers(api_key = "ea918d19ee864a81919bb7b1b98d9aab"),
          query=list(RouteID="70")
          )
```

The code above creates a response object that contains both the raw JSON response from WMATA and also a parsed version that `httr` automatically turned into a list. Let's look at the details of the first bus in the list object.

```{r}
# look at the first bus in the data feed
r70<-content(resp)$BusPositions
r70[[1]]
```

What about the raw JSON?

```{r}
content(resp,"text")
```

That is messy. We can clean it up with the `jsonlite` package.

```{r}
library(jsonlite)
toJSON(r70[[1]],pretty=T)   # convert the list to JSON and "prettify" it
```

Ah, much better. Notice we just turned a list object into JSON text. JSON is a great way to export arbitrary list objects from R to other programs.

Let's save this nicely formatted version of the data-feed to a file for use later.

```r
write_file(toJSON(content(resp),pretty=T),"lecture2/data/route_70.json")
```

To load it later, we can read the file into a character vector and then parse it from JSON to a list using `fromJSON()`.

```r
r70<-fromJSON(read_file("lecture2/data/route_70.json"))
```

### Unstructured / Raw

Unstructured data like [text](http://blogs.lse.ac.uk/businessreview/2016/08/18/businesses-can-no-longer-ignore-social-media-sentiment-analysis/) and [images](http://www.wsj.com/articles/satellites-hedge-funds-eye-in-the-sky-1471207062) are becoming increasingly more important for economic and financial forecasting. We can deal with both of these in R.

#### Text

```{r}
hamlet<-read_file("http://www.gutenberg.org/cache/epub/2265/pg2265.txt")
length(hamlet)     # 1, everything is packed into a single character element
```
If we want individual words, we can split on spaces.

```{r}
words<-unlist(strsplit(hamlet," "))
length(words)
head(words)
```

Many of the words in the text are not unique. We can factor to save space. In text mining, this is called tokenizing.

```{r}
object.size(words)
tokens<-factor(words)
object.size(tokens)
```

That is a little smaller. Let's make everything lowercase so that terms like "The" and "the" are the same token.

```{r}
words<-tolower(words)
tokens<-factor(words)
object.size(tokens)
```

What if we are only interested in "english" words? Let's load a dictionary and remove all the non-dictionary words from Hamlet.

```{r}
en_dict<-read_lines("https://raw.githubusercontent.com/dwyl/english-words/master/words.txt")
en_words <- words[words %in% en_dict]
length(en_words)  # 20223
```

How many of the remaining words have a negative connotation? Let's load a sentiment dictionary and find the share of negative words in Hamlet.

```{r}
neg_dict<-read_lines("http://ptrckprry.com/course/ssd/data/negative-words.txt",skip=35)
sum(en_words %in% neg_dict) / length(en_words)
```

We could repreat this process with another Shakespeare work and compare their sentiment. The accuracy of the estimate would be even better if we could convince a shakespearean literary expert to write a custom dictionary of negatives words for us. But our result is okay for a few minutes of effort.

#### Images

R can load JPEG images using the `jpeg` package.

```{r}
library(jpeg)
four<-readJPEG("data/four.jpeg")
dim(four)       # a 3rd order tensor
```

```{r}
image(four[,,1])
```

The orientation is strange. Take the transpose and/or reverse it looks correct.

```{r}
image(t(four[,,1])[,ncol(four):1])
```

That's a neat trick. Why is this useful? What if we had handwritten digits (a lot of them) and we wanted to decide was number was represented in each image without looking at each one? 

```{r}
digits<-read_csv("data/digits.csv")
names(digits)[1:5]
```
What might a typical 9 look like?

```{r}
label<-digits$label
images<-as.matrix(digits[,-1])
nines<-images[label==9,]
typical_nine<-matrix(colMeans(nines),ncol=28)
image(typical_nine[,28:1])
```

If we were given a new digit image and wanted to know whether or not is was a 9, we could measure its distance from the typical 9 in "pixel space". Comparing this to the distance of other typical digits, we would classify it as a 9 if 9 was the nearest digit.

This is a toy example of image recognition that is an area of machine learning. Hedge Funds have begun using image recognition on satellite imagery of cropland to make financial forecasts ([source](http://www.wsj.com/articles/satellites-hedge-funds-eye-in-the-sky-1471207062)). Other areas of interest might be to look at the signaling of status via luxury goods on instagram.
